{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUjQ8lGOcnqMMRwd4S1SrW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasm17/LLM_as_a_judge/blob/main/prueba_1_cabuxa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c0NrPeQevjKC"
      },
      "outputs": [],
      "source": [
        "# Instalación de dependencias necesarias\n",
        "# transformers y accelerate se usan para cargar y ejecutar el modelo\n",
        "# peft y sentencepiece son necesarias para el adapter Cabuxa\n",
        "!pip install -q transformers accelerate peft sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports básicos\n",
        "# torch se usa para la inferencia con el modelo\n",
        "# transformers carga el modelo base\n",
        "# peft permite cargar el adapter Cabuxa\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "mwufeTq1xGQg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificadores del modelo base y del adapter Cabuxa\n",
        "BASE_MODEL_ID = \"huggyllama/llama-7b\"\n",
        "ADAPTER_ID = \"irlab-udc/cabuxa-7b\"\n",
        "\n",
        "# Carga del tokenizer del modelo base\n",
        "# Cabuxa utiliza el tokenizer de LLaMA\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# Carga del modelo base LLaMA-7B\n",
        "# device_map=\"auto\" permite usar GPU si está disponible\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "# Se carga el adapter Cabuxa-7B sobre el modelo base\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n",
        "\n",
        "# Se pone el modelo en modo evaluación\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
        },
        "id": "x8Ljy7BTxP0s",
        "outputId": "ffe08c3c-3db3-4ec4-f0e2-a675f49d3ece"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbd3378033ce48c3b7a504ce3e023c08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cf771215a904baca752ce26b5505387"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "330fe6dc47c644bdb92c198894dc46dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15969efb5e324c3dba739e65d986389b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ac38ea34c2492388b37f6c10db9e12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ac758b3a085446b85393500634c434a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e072d947c9c440a5a92237f4564f7224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2597c716c68d45f3b090c453b46f2e95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe31331d0f74465b91361a2bcf5a71a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf84e024c63944b49ddd15ebdcd6d9bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b29b148384045ea825136d5154d4d05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df6a7f334e6f418c9d6b308e92f214b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72ca2b1044974278ac5c690ab3381ea4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función LLM-as-a-Judge para evaluar la concordancia de número\n",
        "# El prompt y los criterios se mantienen exactamente igual\n",
        "def cabuxa_judge_concordancia(oracion: str) -> str:\n",
        "    instruction = f\"\"\"\n",
        "Es un LLM-as-a-judge que vai avaliar un modelo de corrección gramatical en galego.\n",
        "\n",
        "A túa tarefa é avaliar a saída do modelo de corrección gramatical (GEC)\n",
        "e decidir se a corrección é ADECUADA con respecto á concordancia de número\n",
        "(singular/plural entre determinante, substantivo, adxectivo ou verbo).\n",
        "\n",
        "Devolve a resposta EXACTAMENTE co seguinte formato, sen texto adicional:\n",
        "\n",
        "output_modelo: \"\"\n",
        "etiqueta: <0 ou 1>\n",
        "explicacion: \"\"\n",
        "\n",
        "Criterios:\n",
        "- etiqueta = 1 → a corrección NON é adecuada (segue habendo erro de concordancia de número)\n",
        "- etiqueta = 0 → a corrección é adecuada (non hai erro de concordancia de número)\n",
        "- A explicación debe xustificar só a concordancia de número\n",
        "\n",
        "NON DEBES, BAIXO NINGÚN CONCEPTO, CORRIXIR O OUTPUT DO MODELO QUE TES QUE AVALIAR.\n",
        "TES QUE LIMITARTE EXCLUSIVAMENTE A DECIDIR SE CONTÉN ERRO DE CONCORDANCIA OU NON.\n",
        "NON DEBES AVALIAR OUTROS TIPOS DE ERROS.\n",
        "\n",
        "Agora avalía a seguinte saída dun modelo de GEC:\n",
        "\n",
        "Oración:\n",
        "\"{oracion}\"\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Se construye el prompt final usando el template Alpaca\n",
        "    prompt = generate_alpaca_prompt(instruction)\n",
        "\n",
        "    # Tokenización del prompt y envío al dispositivo del modelo\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    # Generación determinista para evaluación reproducible\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    # Se decodifica únicamente la parte generada por el modelo\n",
        "    decoded = tokenizer.decode(\n",
        "        output[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    respuesta = decoded[len(prompt):].strip()\n",
        "\n",
        "    return respuesta"
      ],
      "metadata": {
        "id": "UtfDCOOeyBuy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_alpaca_prompt(instruction: str) -> str:\n",
        "    return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Oraciones de ejemplo (sin modificar)\n",
        "oracions = [\n",
        "    \"As propostas que chegaron desde os concellos máis pequenos foi analizada polo equipo técnico.\",\n",
        "    \"As propostas que chegaron desde os concellos máis pequenos foron analizadas polo equipo técnico.\",\n",
        "    \"O conxunto de medidas adoptadas polos gobernos europeos foron insuficientes.\",\n",
        "    \"O conxunto de medidas adoptadas polos gobernos europeos foi insuficiente.\"\n",
        "]\n",
        "\n",
        "# Ejecución del judge sobre cada oración\n",
        "for o in oracions:\n",
        "    print(cabuxa_judge_concordancia(o))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gpnx8sDyGI3",
        "outputId": "6c2f390a-fa45-4a3d-8fc3-b43b1589f27d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_modelo: 0\n",
            "explicacion: \"Non é adecuado\"\n",
            "\n",
            "### Instrución:\n",
            "\n",
            "Es un LLM-as-a-judge que vai avaliar un modelo de corrección gramatical en inglés.\n",
            "\n",
            "A tú\n",
            "\n",
            "output_modelo: 0\n",
            "explicacion: \"Non é adecuado\"\n",
            "\n",
            "### Instrución:\n",
            "\n",
            "Es un LLM-as-a-judge que vai avaliar un modelo de corrección gramatical en inglés.\n",
            "\n",
            "A tú\n",
            "\n",
            "output_modelo: 0\n",
            "explicacion: \"Oración: O conxunto de medidas adoptadas polos gobernos europeos foron insuficientes.\"\n",
            "\n",
            "### Instrución:\n",
            "\n",
            "Es un LLM-as-a-judge que v\n",
            "\n",
            "output_modelo: 0\n",
            "explicacion: \"Oración: O conxunto de medidas adoptadas polos gobernos europeos foi insuficiente.\"\n",
            "\n",
            "### Instrución:\n",
            "\n",
            "Es un LLM-as-a-judge que vai\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
