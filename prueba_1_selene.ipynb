{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H8FvUu7z-EtC"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eYjRQ6LY-TTM"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "assert HF_TOKEN is not None, \"Non se atopou HF_TOKEN nos secrets de Colab\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4p6wQCH2jSG"
      },
      "outputs": [],
      "source": [
        "# Selene-1-Mini-Llama-3.1-8B\n",
        "# LLM-as-a-Judge binario para concordancia de n√∫mero en galego\n",
        "\n",
        "# Cargar modelo e tokenizer\n",
        "MODEL_ID = \"AtlaAI/Selene-1-Mini-Llama-3.1-8B\"\n",
        "\n",
        "print(\"üîÑ Cargando tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "print(\"üîÑ Cargando modelo (pode tardar uns minutos)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ Modelo cargado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptx11U_TFZaG"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n LLM-as-a-Judge (concordancia de n√∫mero)\n",
        "def selene_judge_concordancia(oracion: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "Es un LLM-as-a-judge que vai avaliar un modelo de correcci√≥n gramatical en galego.\n",
        "\n",
        "A t√∫a tarefa √© avaliar a sa√≠da do modelo de correcci√≥n gramatical (GEC)\n",
        "e decidir se a correcci√≥n √© ADECUADA con respecto √° concordancia de n√∫mero\n",
        "(singular/plural entre determinante, substantivo, adxectivo ou verbo).\n",
        "\n",
        "Devolve a resposta EXACTAMENTE co seguinte formato, sen texto adicional:\n",
        "\n",
        "output_modelo: \"<oraci√≥n avaliada>\"\n",
        "etiqueta: <0 ou 1>\n",
        "explicacion: \"<explicaci√≥n breve e precisa en galego>\"\n",
        "\n",
        "Criterios:\n",
        "- etiqueta = 1 ‚Üí a correcci√≥n NON √© adecuada (segue habendo erro de concordancia de n√∫mero)\n",
        "- etiqueta = 0 ‚Üí a correcci√≥n √© adecuada (non hai erro de concordancia de n√∫mero)\n",
        "- A explicaci√≥n debe xustificar s√≥ a concordancia de n√∫mero\n",
        "\n",
        "NON DEBES, BAIXO NING√öN CONCEPTO, CORRIXIR O OUTPUT DO MODELO QUE TES QUE AVALIAR. \n",
        "TESTE QUE LIMITAR EXCLUSIVAMENTE A DECIDIR SE CONT√âN ERRO DE CONCORDANCIA OU NON.\n",
        "NON DEBES AVALIAR OUTROS TIPOS DE ERROS.\n",
        "\n",
        "Agora aval√≠a a seguinte sa√≠da dun modelo de GEC:\n",
        "\n",
        "\"<OUTPUT_MODELO>\"\n",
        "\n",
        "\n",
        "\"{oracion}\"\n",
        "\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    resposta = tokenizer.decode(\n",
        "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return resposta.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5tTG4g0Fe1K",
        "outputId": "2addb883-f960-48c4-fad2-e1abb2b27bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== EXEMPLO 1 =====\n",
            "output_modelo: \"As decisi√≥ns tomadas polo comit√© foron comunicadas aos responsables das distintas √°reas.\"\n",
            "etiqueta: 0\n",
            "explicacion: A concordancia de n√∫mero √© correcta, pois o verbo \"foron\" concorda con o substantivo plural \"decisi√≥ns\".\n",
            "\n",
            "===== EXEMPLO 2 =====\n",
            "output_modelo: \"O grupo de estudantes que participaron no proxecto presentou os resultados finais onte pola tarde.\"\n",
            "etiqueta: 1\n",
            "explicaci√≥n: O verbo \"presentou\" debe ser \"presentaron\" para concordar con o suxeito plural \"estudantes\".\n",
            "\n",
            "===== EXEMPLO 3 =====\n",
            "output_modelo: \"As propostas que chegaron desde os concellos m√°is pequenos foi analizada polo equipo t√©cnico.\"\n",
            "etiqueta: 1\n",
            "explicaci√≥n: O verbo \"foi\" debe ser \"foron\" para concordar co plural do substantivo \"propostas\".\n",
            "\n",
            "===== EXEMPLO 4 =====\n",
            "output_modelo: \"A maior√≠a das persoas entrevistadas manifestaron a s√∫a opini√≥n durante a sesi√≥n p√∫blica.\"\n",
            "etiqueta: 1\n",
            "explicaci√≥n: O verbo \"manifestaron\" non concorda en n√∫mero coa maior√≠a, que √© plural, xa que deber√≠a ser \"manifestaron\" en lugar de \"manifestaron\".\n"
          ]
        }
      ],
      "source": [
        "# Probas cos meus exemplos\n",
        "exemplos = [\n",
        "    \"As decisi√≥ns tomadas polo comit√© foron comunicadas aos responsables das distintas √°reas.\",\n",
        "    \"O grupo de estudantes que participaron no proxecto presentou os resultados finais onte pola tarde.\",\n",
        "    \"As propostas que chegaron desde os concellos m√°is pequenos foi analizada polo equipo t√©cnico.\",\n",
        "    \"A maior√≠a das persoas entrevistadas manifestaron a s√∫a opini√≥n durante a sesi√≥n p√∫blica.\"\n",
        "]\n",
        "\n",
        "for i, frase in enumerate(exemplos, 1):\n",
        "    print(f\"\\n===== EXEMPLO {i} =====\")\n",
        "    print(selene_judge_concordancia(frase))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
