{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRZ6D4l813/iQUDePK+r7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasm17/LLM_as_a_judge/blob/main/prueba_2_prometheus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qnm4Q4osFo9"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias + Imports\n",
        "!pip install -q -U \"transformers>=4.51.0\" accelerate safetensors\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token HF\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)"
      ],
      "metadata": {
        "id": "A6mc0zs4sO4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo e tokenizer (M-Prometheus-7B)\n",
        "# LLM-as-a-Judge binario para concordancia de n√∫mero en galego\n",
        "\n",
        "MODEL_ID = \"Unbabel/M-Prometheus-7B\"\n",
        "\n",
        "print(\"üîÑ Cargando tokenizer...\")\n",
        "if HF_TOKEN:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"üîÑ Cargando modelo (pode tardar uns minutos)...\")\n",
        "if HF_TOKEN:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ Modelo cargado correctamente\")"
      ],
      "metadata": {
        "id": "Rz5L3maIsVGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n LLM-as-a-Judge (concordancia de n√∫mero)\n",
        "def qwen_judge_concordancia(oracion: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are an LLM-as-a-judge evaluating the output of a Galician grammatical error correction (GEC) model.\n",
        "\n",
        "Your task: decide whether the given sentence contains an error of NUMBER AGREEMENT\n",
        "(singular/plural agreement between determiner, noun, adjective, or verb).\n",
        "\n",
        "You MUST evaluate ONLY number agreement. Do NOT evaluate any other error types.\n",
        "You MUST NOT, under any circumstances, correct or rewrite the sentence. Only judge it.\n",
        "\n",
        "Rubric (binary):\n",
        "- [RESULT] 1 -> The correction is NOT adequate (there is still a number agreement error)\n",
        "- [RESULT] 0 -> The correction is adequate (there is NO number agreement error)\n",
        "\n",
        "Output format (STRICT, no extra text):\n",
        "Feedback: \"<brief and precise justification in Galician focused ONLY on number agreement>\"\n",
        "[RESULT] <0 or 1>\n",
        "\n",
        "Now evaluate the following GEC model output:\n",
        "\n",
        "<OUTPUT_MODELO>\n",
        "{oracion}\n",
        "</OUTPUT_MODELO>\n",
        "\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    resposta = tokenizer.decode(\n",
        "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return resposta.strip()"
      ],
      "metadata": {
        "id": "ZvKA1LRdsZOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probas cos exemplos\n",
        "exemplos = [\n",
        "    \"As decisi√≥ns tomadas polo comit√© foron comunicadas aos responsables das distintas √°reas.\",\n",
        "    \"O grupos de estudantes que participou no proxecto presentou os resultados finais onte pola tarde.\",\n",
        "    \"As propostas que chegaron desde os concellos m√°is pequenos foi analizadas polo equipo t√©cnico.\",\n",
        "    \"A maior√≠a das persoas entrevistadas manifestou a s√∫a opini√≥n durante a sesi√≥n p√∫blica.\",\n",
        "    \"Os relatorios anuais sobre o impacto ambiental foi revisados exhaustivamente pola comisi√≥n de avaliaci√≥n.\",\n",
        "    \"A valiosa colecci√≥n de poemas in√©ditos do autor galego publicouse recentemente baixo un prestixioso selo independente.\",\n",
        "    \"O paquete de medidas econ√≥micas recentemente aprobado √© de aplicaci√≥n inmediata en todos os sectores da econom√≠a.\",\n",
        "    \"As adversas condici√≥ns clim√°ticas dos √∫ltimos d√≠as obrigou a suspender completamente varias actividades programadas ao aire libre.\",\n",
        "    \"A maior√≠a dos participantes no curso de formaci√≥n mostrou un grande interese en continuar con sesi√≥ns pr√°cticas adicionais.\",\n",
        "    \"Os sabios consellos que me deches sobre a xesti√≥n do tempo foi moi √∫tiles para poder tomar unha decisi√≥n acertada.\"\n",
        "]\n",
        "\n",
        "for i, frase in enumerate(exemplos, 1):\n",
        "    print(f\"\\n===== EXEMPLO {i} =====\")\n",
        "    print(qwen_judge_concordancia(frase))"
      ],
      "metadata": {
        "id": "LmTFi3jLsfEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}