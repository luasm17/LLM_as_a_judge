{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasm17/LLM_as_a_judge/blob/main/prueba_2_selene.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "H8FvUu7z-EtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "assert HF_TOKEN is not None, \"Non se atopou HF_TOKEN nos secrets de Colab\""
      ],
      "metadata": {
        "id": "eYjRQ6LY-TTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4p6wQCH2jSG"
      },
      "outputs": [],
      "source": [
        "# Selene-1-Mini-Llama-3.1-8B\n",
        "# LLM-as-a-Judge binario para concordancia de n√∫mero en galego\n",
        "\n",
        "# Cargar modelo e tokenizer\n",
        "MODEL_ID = \"AtlaAI/Selene-1-Mini-Llama-3.1-8B\"\n",
        "\n",
        "print(\"üîÑ Cargando tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "print(\"üîÑ Cargando modelo (pode tardar uns minutos)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ Modelo cargado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n LLM-as-a-Judge (concordancia de n√∫mero)\n",
        "def selene_judge_concordancia(oracion: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are an LLM-as-a-judge that will evaluate a grammatical error correction model in Galician.\n",
        "\n",
        "Your task is to evaluate the output of the grammatical error correction (GEC) model\n",
        "and decide whether the correction is ADEQUATE with respect to number agreement\n",
        "(singular/plural between determiner, noun, adjective, or verb).\n",
        "\n",
        "Return the answer EXACTLY in the following format, with no additional text:\n",
        "\n",
        "output_modelo: \"<evaluated sentence>\"\n",
        "etiqueta: <0 or 1>\n",
        "explicacion: \"<brief and precise explanation in Galician>\"\n",
        "\n",
        "Criteria:\n",
        "- etiqueta = 1 ‚Üí the correction is NOT adequate (there is still a number agreement error)\n",
        "- etiqueta = 0 ‚Üí the correction is adequate (there is no number agreement error)\n",
        "- The explanation must justify only the number agreement\n",
        "\n",
        "YOU MUST NOT, UNDER ANY CIRCUMSTANCES, CORRECT THE MODEL OUTPUT YOU HAVE TO EVALUATE. YOU MUST LIMIT YOURSELF EXCLUSIVELY TO DECIDING WHETHER IT CONTAINS A NUMBER AGREEMENT ERROR OR NOT.\n",
        "YOU MUST NOT EVALUATE OTHER TYPES OF ERRORS.\n",
        "\n",
        "Now evaluate the following output from a GEC model:\n",
        "\n",
        "\"<OUTPUT_MODELO>\"\n",
        "\n",
        "\"{oracion}\"\n",
        "\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    resposta = tokenizer.decode(\n",
        "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return resposta.strip()"
      ],
      "metadata": {
        "id": "Ptx11U_TFZaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probas cos meus exemplos\n",
        "exemplos = [\n",
        "    \"As decisi√≥ns tomadas polo comit√© foron comunicadas aos responsables das distintas √°reas.\",\n",
        "    \"O grupos de estudantes que participou no proxecto presentou os resultados finais onte pola tarde.\",\n",
        "    \"As propostas que chegaron desde os concellos m√°is pequenos foi analizadas polo equipo t√©cnico.\",\n",
        "    \"A maior√≠a das persoas entrevistadas manifestou a s√∫a opini√≥n durante a sesi√≥n p√∫blica.\",\n",
        "    \"Os relatorios anuais sobre o impacto ambiental foi revisados exhaustivamente pola comisi√≥n de avaliaci√≥n.\",\n",
        "    \"A valiosa colecci√≥n de poemas in√©ditos do autor galego publicouse recentemente baixo un prestixioso selo independente.\",\n",
        "    \"O paquete de medidas econ√≥micas recentemente aprobado √© de aplicaci√≥n inmediata en todos os sectores da econom√≠a.\",\n",
        "    \"As adversas condici√≥ns clim√°ticas dos √∫ltimos d√≠as obrigou a suspender completamente varias actividades programadas ao aire libre.\",\n",
        "    \"A maior√≠a dos participantes no curso de formaci√≥n mostrou un grande interese en continuar con sesi√≥ns pr√°cticas adicionais.\",\n",
        "    \"Os sabios consellos que me deches sobre a xesti√≥n do tempo foi moi √∫tiles para poder tomar unha decisi√≥n acertada.\",\n",
        "]\n",
        "\n",
        "for i, frase in enumerate(exemplos, 1):\n",
        "    print(f\"\\n===== EXEMPLO {i} =====\")\n",
        "    print(selene_judge_concordancia(frase))"
      ],
      "metadata": {
        "id": "O5tTG4g0Fe1K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}