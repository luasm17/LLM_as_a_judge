{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasm17/LLM_as_a_judge/blob/main/prueba_2_cabuxa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0NrPeQevjKC"
      },
      "outputs": [],
      "source": [
        "# Instalación de dependencias necesarias\n",
        "# transformers y accelerate se usan para cargar y ejecutar el modelo\n",
        "# peft es necesaria para el adapter Cabuxa\n",
        "!pip install -q transformers accelerate peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwufeTq1xGQg"
      },
      "outputs": [],
      "source": [
        "# Imports básicos\n",
        "# torch se usa para la inferencia con el modelo\n",
        "# transformers carga el modelo base\n",
        "# peft permite cargar el adapter Cabuxa\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Ljy7BTxP0s",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Identificadores del modelo base y del adapter Cabuxa\n",
        "BASE_MODEL_ID = \"huggyllama/llama-7b\"\n",
        "ADAPTER_ID = \"irlab-udc/cabuxa-7b\"\n",
        "\n",
        "# Tokenizer correcto\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    use_fast=False\n",
        ")\n",
        "\n",
        "# Token de padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Modelo base\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Aplicar adapter Cabuxa\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    ADAPTER_ID\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cabuxa_prompt(instruction: str, input_text: str | None = None) -> str:\n",
        "    if input_text:\n",
        "        return f\"\"\"Abaixo está unha instrución que describe unha tarefa, xunto cunha entrada que proporciona máis contexto.\n",
        "Escribe unha resposta que responda adecuadamente a entrada.\n",
        "\n",
        "### Instrución:\n",
        "{instruction}\n",
        "\n",
        "### Entrada:\n",
        "{input_text}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está unha instrución que describe unha tarefa.\n",
        "Escribe unha resposta que responda adecuadamente a entrada.\n",
        "\n",
        "### Instrución:\n",
        "{instruction}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "\n",
        "def cabuxa_generate(prompt: str, max_new_tokens: int = 256) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs.get(\"attention_mask\", None)\n",
        "    if attention_mask is not None:\n",
        "        attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0][input_ids.shape[-1]:]  # SOLO lo nuevo\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "def cabuxa_judge_concordancia(oracion: str) -> str:\n",
        "    instruction = \"\"\"\n",
        "You are an LLM-as-a-judge that will evaluate a grammatical error correction model in Galician.\n",
        "\n",
        "Your task is to evaluate the output of the grammatical error correction (GEC) model\n",
        "and decide whether the correction is ADEQUATE with respect to number agreement\n",
        "(singular/plural between determiner, noun, adjective, or verb).\n",
        "\n",
        "Return the answer EXACTLY in the following format, with no additional text:\n",
        "\n",
        "output_modelo: \"<evaluated sentence>\"\n",
        "etiqueta: <0 or 1>\n",
        "explicacion: \"<brief and precise explanation in Galician>\"\n",
        "\n",
        "Criteria:\n",
        "- etiqueta = 1 → the correction is NOT adequate (there is still a number agreement error)\n",
        "- etiqueta = 0 → the correction is adequate (there is no number agreement error)\n",
        "- The explanation must justify only the number agreement\n",
        "\n",
        "YOU MUST NOT, UNDER ANY CIRCUMSTANCES, CORRECT THE MODEL OUTPUT YOU HAVE TO EVALUATE. YOU MUST LIMIT YOURSELF EXCLUSIVELY TO DECIDING WHETHER IT CONTAINS A NUMBER AGREEMENT ERROR OR NOT.\n",
        "YOU MUST NOT EVALUATE OTHER TYPES OF ERRORS.\n",
        "\"\"\".strip()\n",
        "\n",
        "    prompt = generate_cabuxa_prompt(\n",
        "        instruction=instruction,\n",
        "        input_text=f'Oración:\\n\"{oracion}\"'\n",
        "    )\n",
        "\n",
        "    return cabuxa_generate(prompt, max_new_tokens=256)"
      ],
      "metadata": {
        "id": "nOwi1MzTLg0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gpnx8sDyGI3"
      },
      "outputs": [],
      "source": [
        "# Oraciones de ejemplo (sin modificar)\n",
        "exemplos = [\n",
        "    \"As decisións tomadas polo comité foron comunicadas aos responsables das distintas áreas.\",\n",
        "    \"O grupos de estudantes que participou no proxecto presentou os resultados finais onte pola tarde.\",\n",
        "    \"As propostas que chegaron desde os concellos máis pequenos foi analizadas polo equipo técnico.\",\n",
        "    \"A maioría das persoas entrevistadas manifestou a súa opinión durante a sesión pública.\",\n",
        "    \"Os relatorios anuais sobre o impacto ambiental foi revisados exhaustivamente pola comisión de avaliación.\",\n",
        "    \"A valiosa colección de poemas inéditos do autor galego publicouse recentemente baixo un prestixioso selo independente.\",\n",
        "    \"O paquete de medidas económicas recentemente aprobado é de aplicación inmediata en todos os sectores da economía.\",\n",
        "    \"As adversas condicións climáticas dos últimos días obrigou a suspender completamente varias actividades programadas ao aire libre.\",\n",
        "    \"A maioría dos participantes no curso de formación mostrou un grande interese en continuar con sesións prácticas adicionais.\",\n",
        "    \"Os sabios consellos que me deches sobre a xestión do tempo foi moi útiles para poder tomar unha decisión acertada.\",\n",
        "]\n",
        "\n",
        "# Ejecución del judge sobre cada oración\n",
        "for e in exemplos:\n",
        "    print(e)\n",
        "    print(\"→\", cabuxa_judge_concordancia(e))\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}